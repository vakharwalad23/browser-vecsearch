<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<title>Local Embedding & Search Demo</title>
		<style>
			body {
				font-family: Arial, sans-serif;
				background-color: #1a1a1a;
				margin: 0;
				padding: 20px;
				color: #e0e0e0;
			}
			.container {
				max-width: 800px;
				margin: 0 auto;
				background: #2c2c2c;
				padding: 20px;
				border-radius: 8px;
				box-shadow: 0 0 10px rgba(0, 0, 0, 0.5);
			}
			h1,
			h2 {
				color: #ffffff;
			}
			textarea,
			input {
				width: 100%;
				padding: 10px;
				margin-bottom: 10px;
				box-sizing: border-box;
				background-color: #3c3c3c;
				border: 1px solid #4c4c4c;
				color: #e0e0e0;
				border-radius: 4px;
			}
			button {
				padding: 10px 20px;
				background-color: #007bff;
				color: white;
				border: none;
				cursor: pointer;
				border-radius: 4px;
			}
			button:hover {
				background-color: #0056b3;
			}
			.card {
				background-color: #3c3c3c;
				padding: 15px;
				margin-bottom: 10px;
				border-radius: 8px;
				border-left: 4px solid #007bff;
			}
			.progress-bar {
				height: 10px;
				background-color: #007bff;
				border-radius: 5px;
				margin-top: 5px;
			}
			#loading {
				display: none;
				color: #007bff;
				font-weight: bold;
			}
			#error {
				display: none;
				color: #ff4444;
			}
		</style>
		<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.21.0/dist/ort.min.js"></script>
		<script type="module" src="https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2/dist/transformers.min.js"></script>
	</head>
	<body>
		<div class="container">
			<h1>Local embedding & search Demo</h1>
			<p id="loading">Loading model, please wait...</p>
			<p id="error"></p>
			<section id="documents">
				<h2>Documents</h2>
				<textarea id="document-input" placeholder="Add a new document..."></textarea>
				<button id="add-document">Add Document</button>
				<div id="document-list"></div>
			</section>
			<section id="search">
				<h2>Search</h2>
				<input type="text" id="search-input" placeholder="What is AI?" />
				<button id="search-button">Search</button>
				<div id="search-results"></div>
			</section>
		</div>
		<script type="module">
			import { pipeline, env, AutoTokenizer } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2/dist/transformers.min.js';

			env.allowLocalModels = false;
			env.useBrowserCache = true;

			let session;
			const dbName = 'searchDB';
			const storeName = 'documents';
			let db;
			let tokenizer;

			async function tokenizeText(text) {
				if (!tokenizer) {
					console.error('Tokenizer not loaded yet!');
					const maxLength = 128;
					const input_ids = [BigInt(101), BigInt(102), ...Array(maxLength - 2).fill(BigInt(0))];
					const attention_mask = [BigInt(1), BigInt(1), ...Array(maxLength - 2).fill(BigInt(0))];
					return { input_ids, attention_mask };
				}
				const encoded = await tokenizer(text, {
					padding: 'max_length',
					truncation: true,
					max_length: 128,
					return_tensors: 'np',
				});
				const input_ids = BigInt64Array.from(encoded.input_ids.data.map(BigInt));
				const attention_mask = BigInt64Array.from(encoded.attention_mask.data.map(BigInt));
				return { input_ids, attention_mask };
			}

			function normalizeEmbedding(embedding) {
				const norm = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));
				return embedding.map((val) => val / norm);
			}

			const request = indexedDB.open(dbName, 1);
			request.onupgradeneeded = (event) => {
				db = event.target.result;
				db.createObjectStore(storeName, { keyPath: 'id', autoIncrement: true });
			};
			request.onsuccess = (event) => {
				db = event.target.result;
				initialize();
			};
			request.onerror = (event) => {
				console.error('IndexedDB error:', event.target.errorCode);
			};

			async function initialize() {
				const loading = document.getElementById('loading');
				const error = document.getElementById('error');
				loading.style.display = 'block';
				error.style.display = 'none';

				try {
					console.log('Loading tokenizer...');
					tokenizer = await AutoTokenizer.from_pretrained('Xenova/all-MiniLM-L6-v2', {
						quantized: false,
					});
					console.log('Tokenizer loaded successfully');

					console.log('Loading ONNX model...');
					const modelUrl = 'https://browser-vecsearch.davakharwala20.workers.dev/student_embeddings_model_quantized.onnx';
					session = await ort.InferenceSession.create(modelUrl, {
						executionProviders: ['wasm'],
					});
					console.log('ONNX Model loaded successfully');

					console.log('Checking and preloading documents...');
					const transaction = db.transaction([storeName], 'readonly');
					const store = transaction.objectStore(storeName);
					const getAllRequest = store.getAll();

					getAllRequest.onsuccess = async () => {
						const existingDocs = getAllRequest.result.map((doc) => doc.content);
						const preloadedDocs = [
							{
								content:
									'Neural networks draw inspiration from the intricate structure and function of biological brains, implementing artificial neurons and synapses to process information. These interconnected networks can adapt and learn from experience, forming the backbone of modern artificial intelligence systems. From simple perceptrons to complex architectures like convolutional and recurrent neural networks, these bio-inspired systems continue to push the boundaries of what machines can achieve.',
							},
							{
								content:
									"Deep learning, a specialized subset of machine learning, has revolutionized artificial intelligence by mimicking the human brain's neural networks. These sophisticated systems can process and analyze unstructured data like images, videos, and natural language with unprecedented accuracy. Through multiple layers of artificial neurons, deep learning models can automatically learn hierarchical representations of data, enabling breakthroughs in computer vision, speech recognition, and autonomous systems.",
							},
							{
								content:
									'Machine learning algorithms have become increasingly sophisticated in their ability to learn patterns from vast amounts of data. These systems can automatically improve their performance through experience, analyzing complex datasets to identify trends, make predictions, and generate insights that would be impossible for humans to discover manually. From recommendation systems to fraud detection, machine learning is driving innovation across countless applications.',
							},
						];
						const newDocs = preloadedDocs.filter((doc) => !existingDocs.includes(doc.content));

						if (newDocs.length > 0) {
							console.log(`Preloading ${newDocs.length} new documents...`);
							const writeTransaction = db.transaction([storeName], 'readwrite');
							const writeStore = writeTransaction.objectStore(storeName);
							await Promise.all(
								newDocs.map(async (doc) => {
									const { input_ids, attention_mask } = await tokenizeText(doc.content);
									const inputIdsTensor = new ort.Tensor('int64', input_ids, [1, input_ids.length]);
									const attentionMaskTensor = new ort.Tensor('int64', attention_mask, [1, attention_mask.length]);
									const outputs = await session.run({ input_ids: inputIdsTensor, attention_mask: attentionMaskTensor });
									const embedding = normalizeEmbedding(Array.from(outputs.embeddings.data));
									writeStore.add({ content: doc.content, embedding });
								})
							);
							writeTransaction.oncomplete = () => {
								console.log('Preloading complete.');
								loadDocuments();
							};
						} else {
							console.log('No new documents to preload.');
							loadDocuments();
						}
						loading.style.display = 'none';
					};
				} catch (err) {
					console.error('Initialization error:', err);
					loading.style.display = 'none';
					error.style.display = 'block';
					error.textContent = `Error: ${err.message}. Check console for details.`;
				}
			}

			document.getElementById('add-document').addEventListener('click', async () => {
				const text = document.getElementById('document-input').value.trim();
				if (!text || !session || !tokenizer) {
					console.warn('Cannot add document: Text empty, session or tokenizer not ready.');
					return;
				}
				try {
					const { input_ids, attention_mask } = await tokenizeText(text);
					const inputIds = new ort.Tensor('int64', input_ids, [1, input_ids.length]);
					const attentionMask = new ort.Tensor('int64', attention_mask, [1, attention_mask.length]);
					const outputs = await session.run({ input_ids: inputIds, attention_mask: attentionMask });
					const embedding = normalizeEmbedding(Array.from(outputs.embeddings.data));
					const transaction = db.transaction([storeName], 'readwrite');
					const store = transaction.objectStore(storeName);
					store.add({ content: text, embedding });
					transaction.oncomplete = () => {
						document.getElementById('document-input').value = '';
						loadDocuments();
					};
				} catch (error) {
					console.error('Error adding document:', error);
				}
			});

			function loadDocuments() {
				const transaction = db.transaction([storeName], 'readonly');
				const store = transaction.objectStore(storeName);
				const getAllRequest = store.getAll();
				getAllRequest.onsuccess = () => {
					const documents = getAllRequest.result;
					const docList = document.getElementById('document-list');
					docList.innerHTML = '';
					documents.forEach((doc) => {
						const card = document.createElement('div');
						card.className = 'card';
						card.innerHTML = `<p>${doc.content}</p>`;
						docList.appendChild(card);
					});
				};
			}

			function displayResults(results) {
				const resultsDiv = document.getElementById('search-results');
				resultsDiv.innerHTML = ''; // Clear previous results

				if (results.length === 0) {
					resultsDiv.innerHTML = '<p>No results found</p>';
					return;
				}

				results.forEach((result) => {
					const card = document.createElement('div');
					card.className = 'card';

					// Calculate percentage for visual indication of match quality
					const similarityPercent = Math.round(result.similarity * 100);

					card.innerHTML = `
					<p>${result.content}</p>
					<div class="similarity-score">
					  <span>Relevance: ${similarityPercent}%</span>
					  <div class="progress-bar" style="width: ${similarityPercent}%"></div>
					</div>
				  `;
					resultsDiv.appendChild(card);
				});
			}

			document.getElementById('search-button').addEventListener('click', async () => {
				const query = document.getElementById('search-input').value.trim();
				if (!query || !session || !tokenizer) {
					console.warn('Cannot search: Query empty, session or tokenizer not ready.');
					return;
				}

				try {
					// Start timing the entire search operation
					const totalStartTime = performance.now();

					// Time the tokenization
					const tokenizeStartTime = performance.now();
					const { input_ids, attention_mask } = await tokenizeText(query);
					const tokenizeEndTime = performance.now();
					console.log(`Tokenization: ${(tokenizeEndTime - tokenizeStartTime).toFixed(2)}ms`);

					// Time the embedding generation
					const embedStartTime = performance.now();
					const inputIds = new ort.Tensor('int64', input_ids, [1, input_ids.length]);
					const attentionMask = new ort.Tensor('int64', attention_mask, [1, attention_mask.length]);
					const outputs = await session.run({ input_ids: inputIds, attention_mask: attentionMask });
					const queryEmbedding = normalizeEmbedding(Array.from(outputs.embeddings.data));
					const embedEndTime = performance.now();
					console.log(`Embedding generation: ${(embedEndTime - embedStartTime).toFixed(2)}ms`);

					// Time the database access and similarity search
					const searchStartTime = performance.now();
					const transaction = db.transaction([storeName], 'readonly');
					const store = transaction.objectStore(storeName);
					const getAllRequest = store.getAll();

					getAllRequest.onsuccess = () => {
						const dbEndTime = performance.now();
						console.log(`Database access: ${(dbEndTime - searchStartTime).toFixed(2)}ms`);

						// Time the similarity calculations and sorting
						const similarityStartTime = performance.now();
						const documents = getAllRequest.result;
						const results = documents
							.map((doc) => ({
								...doc,
								similarity: fastCosineSimilarity(queryEmbedding, doc.embedding),
							}))
							.sort((a, b) => b.similarity - a.similarity)
							.slice(0, 5);

						const similarityEndTime = performance.now();
						console.log(`Vector similarity calculation: ${(similarityEndTime - similarityStartTime).toFixed(2)}ms`);

						// Time the display rendering
						const renderStartTime = performance.now();
						displayResults(results);
						const renderEndTime = performance.now();
						console.log(`Rendering results: ${(renderEndTime - renderStartTime).toFixed(2)}ms`);

						const totalEndTime = performance.now();
						console.log(`Total search time: ${(totalEndTime - totalStartTime).toFixed(2)}ms`);

						// Add timing to UI
						const resultsDiv = document.getElementById('search-results');
						const timingInfo = document.createElement('div');
						timingInfo.className = 'timing-info';
						timingInfo.innerHTML = `<p>Search completed in ${(totalEndTime - totalStartTime).toFixed(2)}ms</p>`;
						resultsDiv.prepend(timingInfo);
					};
				} catch (error) {
					console.error('Error searching:', error);
				}
			});

			// Add this optimized cosine similarity function (assumes normalized vectors)
			function fastCosineSimilarity(a, b) {
				// When vectors are already normalized, cosine similarity is just the dot product
				return a.reduce((sum, val, i) => sum + val * b[i], 0);
			}

			// Originalnkept for reference
			// function cosineSimilarity(a, b) {
			// const dot = a.reduce((sum, val, i) => sum + val * b[i], 0);
			// const normA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
			// const normB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
			// return normA && normB ? dot / (normA * normB) : 0;
			// }
		</script>
	</body>
</html>
